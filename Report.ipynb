{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aabb0d",
   "metadata": {},
   "source": [
    "# Udacity P2: Continuous Control Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59148923",
   "metadata": {},
   "source": [
    "## 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2878e",
   "metadata": {},
   "source": [
    "This project focuses on solving a continuous control problem using deep reinforcement learning. The goal is to train an agent in the Unity ML-Agents “Reacher” environment, where a robotic arm learns to reach a target position by applying continuous actions. The Deep Deterministic Policy Gradient (DDPG) algorithm was implemented for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19891e39",
   "metadata": {},
   "source": [
    "## 2. Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629e1b9",
   "metadata": {},
   "source": [
    "The agent controls a robotic arm with continuous action space (motor commands) to reach a specified target position.\n",
    "\n",
    "* Observation Space: State vector including positions and velocities (e.g., 33 dimensions).\n",
    "\n",
    "* Action Space: Continuous values (e.g., 4-dimensional vector with values between -1 and 1).\n",
    "\n",
    "* Reward: +0.1 is provided for each step that the agent's hand is in the goal location.\n",
    "\n",
    "* Number of agents: 1 or 20 (It depends on whether the CPU or GPU is trained.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2b2c9",
   "metadata": {},
   "source": [
    "## 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60804e",
   "metadata": {},
   "source": [
    "### 3.1 Learning Algorithm\n",
    "\n",
    "In this project, the **Deep Deterministic Policy Gradient (DDPG)** algorithm was implemented to train the agent.\n",
    "DDPG is an off-policy actor-critic algorithm designed for continuous action spaces. It consists of:\n",
    "\n",
    "* **Actor network:** Proposes continuous actions given a state.\n",
    "\n",
    "* **Critic network:** Estimates the Q-value of state-action pairs to evaluate the actor’s actions.\n",
    "\n",
    "* **Replay buffer:** Stores past experiences to stabilize training.\n",
    "\n",
    "* **Soft target updates:** Gradual update of target networks.\n",
    "\n",
    "* **Exploration noise:** Ornstein-Uhlenbeck process to encourage exploration.\n",
    "\n",
    "**Note:** Adaptive Noise Decay: To encourage exploration during the early stages of training and gradually shift toward exploitation (σ *= 0.995) at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e38e1e",
   "metadata": {},
   "source": [
    "### 3.2 Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0ae79",
   "metadata": {},
   "source": [
    "* **Actor network:** 3 fully connected layers with 400, 300 neurons, ReLU activations; output layer uses tanh activation to keep actions within valid range.\n",
    "\n",
    "* **Critic network:** 3 fully connected layers; the first layer processes state inputs, followed by concatenation with actions and further layers with ReLU activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad3dba",
   "metadata": {},
   "source": [
    "### 3.3 Hyperparameters\n",
    "Hyperparameters with **same** values used for all scenarios:\n",
    "\n",
    "    BATCH_SIZE = 128        # Number of experiences sampled per training step to update the networks.\n",
    "    GAMMA = 0.99            # Discount factor determining how much future rewards are taken into account.\n",
    "    TAU = 1e-3              # Rate at which target networks softly track the learned networks.\n",
    "    WEIGHT_DECAY = 0        # L2 regularization term to prevent overfitting, set to zero here.\n",
    "    sigma_decay = 0.995     # Factor by which exploration noise decreases gradually over time.\n",
    "    min_sigma = 0.05        # Minimum allowed value for noise level to ensure some exploration remains.\n",
    "\n",
    "Hyperparameters with **different** values used for different training scenarios:\n",
    "\n",
    "    BUFFER_SIZE: Maximum number of experiences stored in the replay buffer.\n",
    "    LR_ACTOR: Learning rate used to update the actor (policy) network.\n",
    "    LR_CRITIC: Learning rate used to update the critic (value) network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c700df9",
   "metadata": {},
   "source": [
    "## 4. Plot of Rewards According to Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4b5f2",
   "metadata": {},
   "source": [
    "### Trail 1: Training on cpu with an agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81a8b75c",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(2e5)  # replay buffer size\n",
    "LR_ACTOR = 1e-5         # learning rate of the actor\n",
    "LR_CRITIC = 1e-4        # learning rate of the critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3357152",
   "metadata": {},
   "source": [
    "Environment solved in 1083 episodes!Average Score: 31.08\n",
    "  \n",
    " <img src=\"img/rewards_cpu1.jpg\" style=\"float: left;\"/>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60498e20",
   "metadata": {},
   "source": [
    "### Trail 2: Training on cpu with an agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0b6d50a",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(5e5)  # replay buffer size\n",
    "LR_ACTOR = 1e-6         # learning rate of the actor\n",
    "LR_CRITIC = 1e-5        # learning rate of the critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8538248",
   "metadata": {},
   "source": [
    "Environment solved in 1652 episodes!Average Score: 31.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17852f3f",
   "metadata": {},
   "source": [
    "### Trail 3: Training on gpu with 20 agents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cc66e14",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "BUFFER_SIZE = int(5e5)  # replay buffer size\n",
    "LR_ACTOR = 1e-5         # learning rate of the actor\n",
    "LR_CRITIC = 1e-4        # learning rate of the critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a614301",
   "metadata": {},
   "source": [
    " Environment solved in 111 episodes!\tAverage Score: 30.17\n",
    " \n",
    " <img src=\"img/rewards_gpu.jpg\" style=\"float: left; width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d9113",
   "metadata": {},
   "source": [
    "## 5. Challenges and Solutions\n",
    "\n",
    "* Balancing exploration noise to avoid destabilizing learning.\n",
    "\n",
    "* Tuning network architectures and hyperparameters for better convergence.\n",
    "\n",
    "* Adjusting replay buffer size for effective learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808847a",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "* The agent was able to consistently reach the target within about 100 episodes.\n",
    "\n",
    "* Average rewards increased steadily during training, surpassing the success threshold (+30).\n",
    "\n",
    "* The training process was stable with no significant performance drops.\n",
    "\n",
    "* In the tests, it was observed that models trained by working with 20 agents together generalized the environment better than models trained by only one agent. 20 ajan gpu uzerinde ve bir ajan ise cpu uzerinde farkli bilgisayar sistemlerinde eitilmislerdir. BUFFER_SIZE haricinde ki diger hyperparameterler de aynidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250936a",
   "metadata": {},
   "source": [
    "## 7. Ideas for Future Work\n",
    "\n",
    "* Try advanced algorithms: Implement TD3 (Twin Delayed DDPG) or SAC (Soft Actor-Critic) to improve learning stability and sample efficiency.\n",
    "\n",
    "* Parameter tuning: Experiment with different learning rates, noise parameters, and network sizes to optimize training speed and final performance.\n",
    "\n",
    "* Multi-agent training: Extend the approach to multi-agent environments to test scalability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
